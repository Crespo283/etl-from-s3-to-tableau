[2021-06-06 16:29:06,176] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: forex_data_pipeline.spark_submit_job 2021-06-05T00:00:00+00:00 [queued]>
[2021-06-06 16:29:06,210] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: forex_data_pipeline.spark_submit_job 2021-06-05T00:00:00+00:00 [queued]>
[2021-06-06 16:29:06,214] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2021-06-06 16:29:06,216] {taskinstance.py:1043} INFO - Starting attempt 11 of 12
[2021-06-06 16:29:06,218] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2021-06-06 16:29:06,284] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark_submit_job> on 2021-06-05T00:00:00+00:00
[2021-06-06 16:29:06,306] {standard_task_runner.py:52} INFO - Started process 149 to run task
[2021-06-06 16:29:06,327] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'forex_data_pipeline', 'spark_submit_job', '2021-06-05T00:00:00+00:00', '--job-id', '44', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_spark.py', '--cfg-path', '/tmp/tmpp4bbin9l', '--error-file', '/tmp/tmpm6g6g1ah']
[2021-06-06 16:29:06,353] {standard_task_runner.py:77} INFO - Job 44: Subtask spark_submit_job
[2021-06-06 16:29:06,542] {logging_mixin.py:104} INFO - Running <TaskInstance: forex_data_pipeline.spark_submit_job 2021-06-05T00:00:00+00:00 [running]> on host 72eec67484fb
[2021-06-06 16:29:06,715] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=admin@localhost.com
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=forex_data_pipeline
AIRFLOW_CTX_TASK_ID=spark_submit_job
AIRFLOW_CTX_EXECUTION_DATE=2021-06-05T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-06-05T00:00:00+00:00
[2021-06-06 16:29:06,788] {base.py:74} INFO - Using connection to: id: spark_conn. Host: spark://spark-master, Port: 7077, Schema: , Login: marcelo, Password: XXXXXXXX, extra: None
[2021-06-06 16:29:06,812] {spark_submit.py:364} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --executor-memory 2g --name arrow-spark /opt/airflow/dags/test.py
[2021-06-06 16:29:22,720] {spark_submit.py:526} INFO - WARNING: An illegal reflective access operation has occurred
[2021-06-06 16:29:22,769] {spark_submit.py:526} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/airflow/.local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2021-06-06 16:29:22,790] {spark_submit.py:526} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2021-06-06 16:29:22,816] {spark_submit.py:526} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2021-06-06 16:29:22,858] {spark_submit.py:526} INFO - WARNING: All illegal access operations will be denied in a future release
[2021-06-06 16:29:27,151] {spark_submit.py:526} INFO - 21/06/06 16:29:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-06-06 16:29:36,573] {spark_submit.py:526} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-06-06 16:29:36,607] {spark_submit.py:526} INFO - 21/06/06 16:29:36 INFO SparkContext: Running Spark version 3.1.2
[2021-06-06 16:29:36,787] {spark_submit.py:526} INFO - 21/06/06 16:29:36 INFO ResourceUtils: ==============================================================
[2021-06-06 16:29:36,905] {spark_submit.py:526} INFO - 21/06/06 16:29:36 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-06-06 16:29:36,912] {spark_submit.py:526} INFO - 21/06/06 16:29:36 INFO ResourceUtils: ==============================================================
[2021-06-06 16:29:36,928] {spark_submit.py:526} INFO - 21/06/06 16:29:36 INFO SparkContext: Submitted application: Forex processing
[2021-06-06 16:29:37,078] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-06-06 16:29:37,175] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO ResourceProfile: Limiting resource is cpu
[2021-06-06 16:29:37,185] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-06-06 16:29:37,570] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO SecurityManager: Changing view acls to: airflow
[2021-06-06 16:29:37,573] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO SecurityManager: Changing modify acls to: airflow
[2021-06-06 16:29:37,575] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO SecurityManager: Changing view acls groups to:
[2021-06-06 16:29:37,581] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO SecurityManager: Changing modify acls groups to:
[2021-06-06 16:29:37,639] {spark_submit.py:526} INFO - 21/06/06 16:29:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2021-06-06 16:29:40,652] {spark_submit.py:526} INFO - 21/06/06 16:29:40 INFO Utils: Successfully started service 'sparkDriver' on port 33697.
[2021-06-06 16:29:40,887] {spark_submit.py:526} INFO - 21/06/06 16:29:40 INFO SparkEnv: Registering MapOutputTracker
[2021-06-06 16:29:41,153] {spark_submit.py:526} INFO - 21/06/06 16:29:41 INFO SparkEnv: Registering BlockManagerMaster
[2021-06-06 16:29:41,436] {spark_submit.py:526} INFO - 21/06/06 16:29:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-06-06 16:29:41,444] {spark_submit.py:526} INFO - 21/06/06 16:29:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-06-06 16:29:41,528] {spark_submit.py:526} INFO - 21/06/06 16:29:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-06-06 16:29:41,659] {spark_submit.py:526} INFO - 21/06/06 16:29:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6456dd1a-ae4f-4feb-94a0-6161341e6370
[2021-06-06 16:29:41,968] {spark_submit.py:526} INFO - 21/06/06 16:29:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2021-06-06 16:29:42,192] {spark_submit.py:526} INFO - 21/06/06 16:29:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-06-06 16:29:44,975] {spark_submit.py:526} INFO - 21/06/06 16:29:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-06-06 16:29:46,208] {spark_submit.py:526} INFO - 21/06/06 16:29:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://72eec67484fb:4040
[2021-06-06 16:29:49,093] {spark_submit.py:526} INFO - 21/06/06 16:29:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2021-06-06 16:29:49,547] {spark_submit.py:526} INFO - 21/06/06 16:29:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.5:7077 after 242 ms (0 ms spent in bootstraps)
[2021-06-06 16:29:51,417] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210606162951-0000
[2021-06-06 16:29:51,532] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43453.
[2021-06-06 16:29:51,536] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO NettyBlockTransferService: Server created on 72eec67484fb:43453
[2021-06-06 16:29:51,542] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-06-06 16:29:51,774] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 72eec67484fb, 43453, None)
[2021-06-06 16:29:51,953] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO BlockManagerMasterEndpoint: Registering block manager 72eec67484fb:43453 with 434.4 MiB RAM, BlockManagerId(driver, 72eec67484fb, 43453, None)
[2021-06-06 16:29:52,016] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 72eec67484fb, 43453, None)
[2021-06-06 16:29:52,028] {spark_submit.py:526} INFO - 21/06/06 16:29:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 72eec67484fb, 43453, None)
[2021-06-06 16:29:54,340] {spark_submit.py:526} INFO - 21/06/06 16:29:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2021-06-06 16:29:56,187] {spark_submit.py:526} INFO - 21/06/06 16:29:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/opt/airflow/spark-warehouse').
[2021-06-06 16:29:56,190] {spark_submit.py:526} INFO - 21/06/06 16:29:56 INFO SharedState: Warehouse path is '/opt/airflow/spark-warehouse'.
[2021-06-06 16:30:36,295] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO CodeGenerator: Code generated in 3054.18626 ms
[2021-06-06 16:30:36,765] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2021-06-06 16:30:36,822] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-06-06 16:30:36,838] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2021-06-06 16:30:36,878] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO DAGScheduler: Parents of final stage: List()
[2021-06-06 16:30:36,882] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO DAGScheduler: Missing parents: List()
[2021-06-06 16:30:36,888] {spark_submit.py:526} INFO - 21/06/06 16:30:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-06-06 16:30:37,793] {spark_submit.py:526} INFO - 21/06/06 16:30:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.1 KiB, free 434.4 MiB)
[2021-06-06 16:30:38,048] {spark_submit.py:526} INFO - 21/06/06 16:30:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.4 MiB)
[2021-06-06 16:30:38,121] {spark_submit.py:526} INFO - 21/06/06 16:30:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 72eec67484fb:43453 (size: 5.8 KiB, free: 434.4 MiB)
[2021-06-06 16:30:38,169] {spark_submit.py:526} INFO - 21/06/06 16:30:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388
[2021-06-06 16:30:38,376] {spark_submit.py:526} INFO - 21/06/06 16:30:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-06-06 16:30:38,408] {spark_submit.py:526} INFO - 21/06/06 16:30:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-06-06 16:30:53,526] {spark_submit.py:526} INFO - 21/06/06 16:30:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:31:08,515] {spark_submit.py:526} INFO - 21/06/06 16:31:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:31:23,533] {spark_submit.py:526} INFO - 21/06/06 16:31:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:31:38,519] {spark_submit.py:526} INFO - 21/06/06 16:31:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:31:53,573] {spark_submit.py:526} INFO - 21/06/06 16:31:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:32:08,518] {spark_submit.py:526} INFO - 21/06/06 16:32:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:32:23,517] {spark_submit.py:526} INFO - 21/06/06 16:32:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:32:38,518] {spark_submit.py:526} INFO - 21/06/06 16:32:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:32:53,539] {spark_submit.py:526} INFO - 21/06/06 16:32:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:33:08,565] {spark_submit.py:526} INFO - 21/06/06 16:33:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:33:23,533] {spark_submit.py:526} INFO - 21/06/06 16:33:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:33:38,515] {spark_submit.py:526} INFO - 21/06/06 16:33:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:33:53,515] {spark_submit.py:526} INFO - 21/06/06 16:33:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:34:08,515] {spark_submit.py:526} INFO - 21/06/06 16:34:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:34:23,517] {spark_submit.py:526} INFO - 21/06/06 16:34:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:34:38,515] {spark_submit.py:526} INFO - 21/06/06 16:34:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:34:53,516] {spark_submit.py:526} INFO - 21/06/06 16:34:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:35:08,572] {spark_submit.py:526} INFO - 21/06/06 16:35:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:35:23,516] {spark_submit.py:526} INFO - 21/06/06 16:35:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:35:38,515] {spark_submit.py:526} INFO - 21/06/06 16:35:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:35:53,531] {spark_submit.py:526} INFO - 21/06/06 16:35:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:36:08,518] {spark_submit.py:526} INFO - 21/06/06 16:36:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:36:23,527] {spark_submit.py:526} INFO - 21/06/06 16:36:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:36:38,517] {spark_submit.py:526} INFO - 21/06/06 16:36:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:36:53,515] {spark_submit.py:526} INFO - 21/06/06 16:36:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:37:08,515] {spark_submit.py:526} INFO - 21/06/06 16:37:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:37:23,591] {spark_submit.py:526} INFO - 21/06/06 16:37:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:37:38,519] {spark_submit.py:526} INFO - 21/06/06 16:37:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:37:53,518] {spark_submit.py:526} INFO - 21/06/06 16:37:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:38:08,515] {spark_submit.py:526} INFO - 21/06/06 16:38:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:38:23,516] {spark_submit.py:526} INFO - 21/06/06 16:38:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:38:38,536] {spark_submit.py:526} INFO - 21/06/06 16:38:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:38:53,522] {spark_submit.py:526} INFO - 21/06/06 16:38:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:39:08,521] {spark_submit.py:526} INFO - 21/06/06 16:39:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:39:23,515] {spark_submit.py:526} INFO - 21/06/06 16:39:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:39:38,515] {spark_submit.py:526} INFO - 21/06/06 16:39:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:39:53,516] {spark_submit.py:526} INFO - 21/06/06 16:39:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:40:08,515] {spark_submit.py:526} INFO - 21/06/06 16:40:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:40:23,521] {spark_submit.py:526} INFO - 21/06/06 16:40:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:40:38,516] {spark_submit.py:526} INFO - 21/06/06 16:40:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:40:53,518] {spark_submit.py:526} INFO - 21/06/06 16:40:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:41:08,520] {spark_submit.py:526} INFO - 21/06/06 16:41:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:41:23,516] {spark_submit.py:526} INFO - 21/06/06 16:41:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:41:38,517] {spark_submit.py:526} INFO - 21/06/06 16:41:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:41:53,520] {spark_submit.py:526} INFO - 21/06/06 16:41:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 16:41:55,078] {local_task_job.py:188} WARNING - State of this instance has been externally set to shutdown. Terminating instance.
[2021-06-06 16:41:55,264] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 149
[2021-06-06 16:41:55,366] {taskinstance.py:1239} ERROR - Received SIGTERM. Terminating subprocesses.
[2021-06-06 16:41:56,495] {spark_submit.py:657} INFO - Sending kill signal to spark-submit
[2021-06-06 16:41:57,538] {taskinstance.py:1455} ERROR - Task received SIGTERM signal
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 183, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 440, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 494, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1241, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2021-06-06 16:41:58,159] {taskinstance.py:1503} INFO - Marking task as FAILED. dag_id=forex_data_pipeline, task_id=spark_submit_job, execution_date=20210605T000000, start_date=20210606T162906, end_date=20210606T164157
[2021-06-06 16:42:00,536] {process_utils.py:66} INFO - Process psutil.Process(pid=149, status='terminated', exitcode=1, started='16:29:05') (149) terminated with exit code 1
[2021-06-06 16:42:02,333] {process_utils.py:66} INFO - Process psutil.Process(pid=150, status='terminated', started='16:29:06') (150) terminated with exit code None
[2021-06-06 16:42:02,366] {process_utils.py:66} INFO - Process psutil.Process(pid=220, status='terminated', started='16:29:30') (220) terminated with exit code None
