[2021-06-06 16:58:06,134] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: forex_data_pipeline.spark_submit_job 2021-06-05T00:00:00+00:00 [queued]>
[2021-06-06 16:58:06,334] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: forex_data_pipeline.spark_submit_job 2021-06-05T00:00:00+00:00 [queued]>
[2021-06-06 16:58:06,364] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2021-06-06 16:58:06,369] {taskinstance.py:1043} INFO - Starting attempt 13 of 14
[2021-06-06 16:58:06,373] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2021-06-06 16:58:06,421] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark_submit_job> on 2021-06-05T00:00:00+00:00
[2021-06-06 16:58:06,495] {standard_task_runner.py:52} INFO - Started process 157 to run task
[2021-06-06 16:58:06,522] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'forex_data_pipeline', 'spark_submit_job', '2021-06-05T00:00:00+00:00', '--job-id', '47', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_spark.py', '--cfg-path', '/tmp/tmp1n4zr6tv', '--error-file', '/tmp/tmp7rho2o8o']
[2021-06-06 16:58:06,569] {standard_task_runner.py:77} INFO - Job 47: Subtask spark_submit_job
[2021-06-06 16:58:07,320] {logging_mixin.py:104} INFO - Running <TaskInstance: forex_data_pipeline.spark_submit_job 2021-06-05T00:00:00+00:00 [running]> on host 72eec67484fb
[2021-06-06 16:58:07,812] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=admin@localhost.com
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=forex_data_pipeline
AIRFLOW_CTX_TASK_ID=spark_submit_job
AIRFLOW_CTX_EXECUTION_DATE=2021-06-05T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-06-05T00:00:00+00:00
[2021-06-06 16:58:08,016] {base.py:74} INFO - Using connection to: id: spark_conn. Host: spark://spark-master, Port: 7077, Schema: , Login: marcelo, Password: XXXXXXXX, extra: None
[2021-06-06 16:58:08,042] {spark_submit.py:364} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.dynamicAllocation.enabled=false --conf spark.shuffle.service.enabled=false --num-executors 1 --total-executor-cores 1 --executor-cores 1 --executor-memory 2g --driver-memory 2g --name airflow-spark1 /opt/airflow/dags/test.py
[2021-06-06 16:58:23,977] {spark_submit.py:526} INFO - WARNING: An illegal reflective access operation has occurred
[2021-06-06 16:58:23,992] {spark_submit.py:526} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/airflow/.local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2021-06-06 16:58:24,003] {spark_submit.py:526} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2021-06-06 16:58:24,034] {spark_submit.py:526} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2021-06-06 16:58:24,052] {spark_submit.py:526} INFO - WARNING: All illegal access operations will be denied in a future release
[2021-06-06 16:58:26,906] {spark_submit.py:526} INFO - 21/06/06 16:58:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-06-06 16:58:34,570] {spark_submit.py:526} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-06-06 16:58:34,654] {spark_submit.py:526} INFO - 21/06/06 16:58:34 INFO SparkContext: Running Spark version 3.1.2
[2021-06-06 16:58:35,302] {spark_submit.py:526} INFO - 21/06/06 16:58:35 INFO ResourceUtils: ==============================================================
[2021-06-06 16:58:35,357] {spark_submit.py:526} INFO - 21/06/06 16:58:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-06-06 16:58:35,366] {spark_submit.py:526} INFO - 21/06/06 16:58:35 INFO ResourceUtils: ==============================================================
[2021-06-06 16:58:35,371] {spark_submit.py:526} INFO - 21/06/06 16:58:35 INFO SparkContext: Submitted application: Forex processing
[2021-06-06 16:58:35,689] {spark_submit.py:526} INFO - 21/06/06 16:58:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-06-06 16:58:36,114] {spark_submit.py:526} INFO - 21/06/06 16:58:36 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
[2021-06-06 16:58:36,139] {spark_submit.py:526} INFO - 21/06/06 16:58:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-06-06 16:58:37,255] {spark_submit.py:526} INFO - 21/06/06 16:58:37 INFO SecurityManager: Changing view acls to: airflow
[2021-06-06 16:58:37,301] {spark_submit.py:526} INFO - 21/06/06 16:58:37 INFO SecurityManager: Changing modify acls to: airflow
[2021-06-06 16:58:37,305] {spark_submit.py:526} INFO - 21/06/06 16:58:37 INFO SecurityManager: Changing view acls groups to:
[2021-06-06 16:58:37,334] {spark_submit.py:526} INFO - 21/06/06 16:58:37 INFO SecurityManager: Changing modify acls groups to:
[2021-06-06 16:58:37,368] {spark_submit.py:526} INFO - 21/06/06 16:58:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2021-06-06 16:58:40,496] {spark_submit.py:526} INFO - 21/06/06 16:58:40 INFO Utils: Successfully started service 'sparkDriver' on port 39669.
[2021-06-06 16:58:41,071] {spark_submit.py:526} INFO - 21/06/06 16:58:41 INFO SparkEnv: Registering MapOutputTracker
[2021-06-06 16:58:41,413] {spark_submit.py:526} INFO - 21/06/06 16:58:41 INFO SparkEnv: Registering BlockManagerMaster
[2021-06-06 16:58:41,784] {spark_submit.py:526} INFO - 21/06/06 16:58:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-06-06 16:58:41,815] {spark_submit.py:526} INFO - 21/06/06 16:58:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-06-06 16:58:41,842] {spark_submit.py:526} INFO - 21/06/06 16:58:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-06-06 16:58:42,223] {spark_submit.py:526} INFO - 21/06/06 16:58:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9caafef3-523a-4fbd-8a0b-4e7b38cadc19
[2021-06-06 16:58:42,526] {spark_submit.py:526} INFO - 21/06/06 16:58:42 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
[2021-06-06 16:58:42,969] {spark_submit.py:526} INFO - 21/06/06 16:58:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-06-06 16:58:46,172] {spark_submit.py:526} INFO - 21/06/06 16:58:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-06-06 16:58:48,443] {spark_submit.py:526} INFO - 21/06/06 16:58:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://72eec67484fb:4040
[2021-06-06 16:58:52,846] {spark_submit.py:526} INFO - 21/06/06 16:58:52 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2021-06-06 16:58:53,138] {spark_submit.py:526} INFO - 21/06/06 16:58:53 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.6:7077 after 148 ms (0 ms spent in bootstraps)
[2021-06-06 16:58:56,439] {spark_submit.py:526} INFO - 21/06/06 16:58:56 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210606165855-0000
[2021-06-06 16:58:56,881] {spark_submit.py:526} INFO - 21/06/06 16:58:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35115.
[2021-06-06 16:58:56,896] {spark_submit.py:526} INFO - 21/06/06 16:58:56 INFO NettyBlockTransferService: Server created on 72eec67484fb:35115
[2021-06-06 16:58:56,903] {spark_submit.py:526} INFO - 21/06/06 16:58:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-06-06 16:58:56,998] {spark_submit.py:526} INFO - 21/06/06 16:58:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 72eec67484fb, 35115, None)
[2021-06-06 16:58:57,118] {spark_submit.py:526} INFO - 21/06/06 16:58:57 INFO BlockManagerMasterEndpoint: Registering block manager 72eec67484fb:35115 with 1048.8 MiB RAM, BlockManagerId(driver, 72eec67484fb, 35115, None)
[2021-06-06 16:58:57,138] {spark_submit.py:526} INFO - 21/06/06 16:58:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 72eec67484fb, 35115, None)
[2021-06-06 16:58:57,204] {spark_submit.py:526} INFO - 21/06/06 16:58:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 72eec67484fb, 35115, None)
[2021-06-06 16:58:58,873] {spark_submit.py:526} INFO - 21/06/06 16:58:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2021-06-06 16:59:00,373] {spark_submit.py:526} INFO - 21/06/06 16:59:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/opt/airflow/spark-warehouse').
[2021-06-06 16:59:00,380] {spark_submit.py:526} INFO - 21/06/06 16:59:00 INFO SharedState: Warehouse path is '/opt/airflow/spark-warehouse'.
[2021-06-06 16:59:33,673] {spark_submit.py:526} INFO - 21/06/06 16:59:33 INFO CodeGenerator: Code generated in 1857.71 ms
[2021-06-06 16:59:33,858] {spark_submit.py:526} INFO - 21/06/06 16:59:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2021-06-06 16:59:34,218] {spark_submit.py:526} INFO - 21/06/06 16:59:34 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-06-06 16:59:34,247] {spark_submit.py:526} INFO - 21/06/06 16:59:34 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2021-06-06 16:59:34,268] {spark_submit.py:526} INFO - 21/06/06 16:59:34 INFO DAGScheduler: Parents of final stage: List()
[2021-06-06 16:59:34,273] {spark_submit.py:526} INFO - 21/06/06 16:59:34 INFO DAGScheduler: Missing parents: List()
[2021-06-06 16:59:34,283] {spark_submit.py:526} INFO - 21/06/06 16:59:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-06-06 16:59:35,237] {spark_submit.py:526} INFO - 21/06/06 16:59:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.1 KiB, free 1048.8 MiB)
[2021-06-06 16:59:35,549] {spark_submit.py:526} INFO - 21/06/06 16:59:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 1048.8 MiB)
[2021-06-06 16:59:36,009] {spark_submit.py:526} INFO - 21/06/06 16:59:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 72eec67484fb:35115 (size: 5.8 KiB, free: 1048.8 MiB)
[2021-06-06 16:59:36,144] {spark_submit.py:526} INFO - 21/06/06 16:59:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388
[2021-06-06 16:59:36,391] {spark_submit.py:526} INFO - 21/06/06 16:59:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-06-06 16:59:36,395] {spark_submit.py:526} INFO - 21/06/06 16:59:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-06-06 16:59:51,586] {spark_submit.py:526} INFO - 21/06/06 16:59:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:00:06,583] {spark_submit.py:526} INFO - 21/06/06 17:00:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:00:21,583] {spark_submit.py:526} INFO - 21/06/06 17:00:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:00:36,588] {spark_submit.py:526} INFO - 21/06/06 17:00:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:00:51,726] {spark_submit.py:526} INFO - 21/06/06 17:00:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:01:06,584] {spark_submit.py:526} INFO - 21/06/06 17:01:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:01:21,584] {spark_submit.py:526} INFO - 21/06/06 17:01:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:01:36,639] {spark_submit.py:526} INFO - 21/06/06 17:01:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:01:51,822] {spark_submit.py:526} INFO - 21/06/06 17:01:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:02:06,661] {spark_submit.py:526} INFO - 21/06/06 17:02:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:02:21,584] {spark_submit.py:526} INFO - 21/06/06 17:02:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:02:36,585] {spark_submit.py:526} INFO - 21/06/06 17:02:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:02:51,602] {spark_submit.py:526} INFO - 21/06/06 17:02:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:03:06,586] {spark_submit.py:526} INFO - 21/06/06 17:03:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:03:21,635] {spark_submit.py:526} INFO - 21/06/06 17:03:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:03:36,588] {spark_submit.py:526} INFO - 21/06/06 17:03:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:03:51,601] {spark_submit.py:526} INFO - 21/06/06 17:03:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:04:06,585] {spark_submit.py:526} INFO - 21/06/06 17:04:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:04:21,590] {spark_submit.py:526} INFO - 21/06/06 17:04:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:04:36,584] {spark_submit.py:526} INFO - 21/06/06 17:04:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:04:51,588] {spark_submit.py:526} INFO - 21/06/06 17:04:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:05:06,586] {spark_submit.py:526} INFO - 21/06/06 17:05:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:05:20,547] {spark_submit.py:526} INFO - 21/06/06 17:05:20 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED
[2021-06-06 17:05:20,922] {spark_submit.py:526} INFO - 21/06/06 17:05:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2021-06-06 17:05:21,608] {spark_submit.py:526} INFO - 21/06/06 17:05:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 17:05:21,654] {spark_submit.py:526} INFO - 21/06/06 17:05:21 INFO TaskSchedulerImpl: Cancelling stage 0
[2021-06-06 17:05:21,792] {spark_submit.py:526} INFO - 21/06/06 17:05:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
[2021-06-06 17:05:22,034] {spark_submit.py:526} INFO - 21/06/06 17:05:22 INFO SparkUI: Stopped Spark web UI at http://72eec67484fb:4040
[2021-06-06 17:05:22,064] {spark_submit.py:526} INFO - 21/06/06 17:05:22 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) failed in 347.433 s due to Job aborted due to stage failure: Master removed our application: KILLED
[2021-06-06 17:05:22,253] {spark_submit.py:526} INFO - 21/06/06 17:05:22 INFO DAGScheduler: Job 0 failed: showString at NativeMethodAccessorImpl.java:0, took 348.776937 s
[2021-06-06 17:05:22,336] {spark_submit.py:526} INFO - 21/06/06 17:05:22 INFO StandaloneSchedulerBackend: Shutting down all executors
[2021-06-06 17:05:22,634] {spark_submit.py:526} INFO - 21/06/06 17:05:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
[2021-06-06 17:05:23,202] {spark_submit.py:526} INFO - 21/06/06 17:05:22 ERROR Utils: Uncaught exception in thread stop-spark-context
[2021-06-06 17:05:23,269] {spark_submit.py:526} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2021-06-06 17:05:23,345] {spark_submit.py:526} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2021-06-06 17:05:23,352] {spark_submit.py:526} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2021-06-06 17:05:23,430] {spark_submit.py:526} INFO - at org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:287)
[2021-06-06 17:05:23,457] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:259)
[2021-06-06 17:05:23,478] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:131)
[2021-06-06 17:05:23,525] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:881)
[2021-06-06 17:05:23,564] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2370)
[2021-06-06 17:05:23,611] {spark_submit.py:526} INFO - at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2069)
[2021-06-06 17:05:23,621] {spark_submit.py:526} INFO - at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)
[2021-06-06 17:05:23,637] {spark_submit.py:526} INFO - at org.apache.spark.SparkContext.stop(SparkContext.scala:2069)
[2021-06-06 17:05:23,666] {spark_submit.py:526} INFO - at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2018)
[2021-06-06 17:05:23,696] {spark_submit.py:526} INFO - Caused by: org.apache.spark.SparkException: Could not find AppClient.
[2021-06-06 17:05:23,753] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
[2021-06-06 17:05:23,776] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
[2021-06-06 17:05:23,803] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
[2021-06-06 17:05:23,960] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
[2021-06-06 17:05:23,969] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
[2021-06-06 17:05:24,018] {spark_submit.py:526} INFO - at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:74)
[2021-06-06 17:05:24,031] {spark_submit.py:526} INFO - ... 9 more
[2021-06-06 17:05:24,071] {spark_submit.py:526} INFO - Traceback (most recent call last):
[2021-06-06 17:05:24,143] {spark_submit.py:526} INFO - File "/opt/airflow/dags/test.py", line 33, in <module>
[2021-06-06 17:05:24,164] {spark_submit.py:526} INFO - print(dfFromData2.show(5))
[2021-06-06 17:05:24,283] {spark_submit.py:526} INFO - File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 484, in show
[2021-06-06 17:05:24,490] {spark_submit.py:526} INFO - 21/06/06 17:05:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2021-06-06 17:05:24,586] {spark_submit.py:526} INFO - File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
[2021-06-06 17:05:24,600] {spark_submit.py:526} INFO - File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2021-06-06 17:05:24,656] {spark_submit.py:526} INFO - File "/home/airflow/.local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value
[2021-06-06 17:05:24,696] {spark_submit.py:526} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o104.showString.
[2021-06-06 17:05:24,719] {spark_submit.py:526} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED
[2021-06-06 17:05:24,813] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
[2021-06-06 17:05:24,827] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
[2021-06-06 17:05:25,039] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
[2021-06-06 17:05:25,121] {spark_submit.py:526} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2021-06-06 17:05:25,161] {spark_submit.py:526} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2021-06-06 17:05:25,275] {spark_submit.py:526} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2021-06-06 17:05:25,288] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
[2021-06-06 17:05:25,332] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
[2021-06-06 17:05:25,423] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
[2021-06-06 17:05:25,470] {spark_submit.py:526} INFO - at scala.Option.foreach(Option.scala:407)
[2021-06-06 17:05:25,542] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
[2021-06-06 17:05:25,692] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
[2021-06-06 17:05:25,812] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
[2021-06-06 17:05:25,889] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
[2021-06-06 17:05:25,938] {spark_submit.py:526} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2021-06-06 17:05:26,078] {spark_submit.py:526} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
[2021-06-06 17:05:26,151] {spark_submit.py:526} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
[2021-06-06 17:05:26,177] {spark_submit.py:526} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
[2021-06-06 17:05:26,182] {spark_submit.py:526} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
[2021-06-06 17:05:26,221] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)
[2021-06-06 17:05:26,281] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)
[2021-06-06 17:05:26,319] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
[2021-06-06 17:05:26,335] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)
[2021-06-06 17:05:26,451] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)
[2021-06-06 17:05:26,610] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
[2021-06-06 17:05:26,660] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2021-06-06 17:05:26,934] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2021-06-06 17:05:26,976] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2021-06-06 17:05:27,120] {spark_submit.py:526} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2021-06-06 17:05:27,203] {spark_submit.py:526} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2021-06-06 17:05:27,273] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
[2021-06-06 17:05:27,278] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)
[2021-06-06 17:05:27,371] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)
[2021-06-06 17:05:27,420] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)
[2021-06-06 17:05:27,437] {spark_submit.py:526} INFO - at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)
[2021-06-06 17:05:27,542] {spark_submit.py:526} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2021-06-06 17:05:27,654] {spark_submit.py:526} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2021-06-06 17:05:27,739] {spark_submit.py:526} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2021-06-06 17:05:27,786] {spark_submit.py:526} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2021-06-06 17:05:28,023] {spark_submit.py:526} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2021-06-06 17:05:28,034] {spark_submit.py:526} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2021-06-06 17:05:28,067] {spark_submit.py:526} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2021-06-06 17:05:28,318] {spark_submit.py:526} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2021-06-06 17:05:28,399] {spark_submit.py:526} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2021-06-06 17:05:28,450] {spark_submit.py:526} INFO - at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2021-06-06 17:05:28,497] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 17:05:28,534] {spark_submit.py:526} INFO - 
[2021-06-06 17:05:29,646] {spark_submit.py:526} INFO - 21/06/06 17:05:29 INFO MemoryStore: MemoryStore cleared
[2021-06-06 17:05:29,654] {spark_submit.py:526} INFO - 21/06/06 17:05:29 INFO BlockManager: BlockManager stopped
[2021-06-06 17:05:33,163] {spark_submit.py:526} INFO - 21/06/06 17:05:33 INFO BlockManagerMaster: BlockManagerMaster stopped
[2021-06-06 17:05:33,374] {spark_submit.py:526} INFO - 21/06/06 17:05:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2021-06-06 17:05:33,786] {spark_submit.py:526} INFO - 21/06/06 17:05:33 INFO SparkContext: Successfully stopped SparkContext
[2021-06-06 17:05:34,605] {spark_submit.py:526} INFO - 21/06/06 17:05:34 INFO ShutdownHookManager: Shutdown hook called
[2021-06-06 17:05:34,648] {spark_submit.py:526} INFO - 21/06/06 17:05:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4948f77-07a4-4dec-bac5-c5c7e5e8c315
[2021-06-06 17:05:34,678] {spark_submit.py:526} INFO - 21/06/06 17:05:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-b900a3fe-6d37-4636-b28b-7070ac4d1f53
[2021-06-06 17:05:34,724] {spark_submit.py:526} INFO - 21/06/06 17:05:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-b900a3fe-6d37-4636-b28b-7070ac4d1f53/pyspark-946087bc-19ab-4f7b-bd4c-19a126839578
[2021-06-06 17:05:48,975] {taskinstance.py:1455} ERROR - Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.dynamicAllocation.enabled=false --conf spark.shuffle.service.enabled=false --num-executors 1 --total-executor-cores 1 --executor-cores 1 --executor-memory 2g --driver-memory 2g --name airflow-spark1 /opt/airflow/dags/test.py. Error code is: 1.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 183, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 455, in submit
    self._mask_cmd(spark_submit_cmd), returncode
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.dynamicAllocation.enabled=false --conf spark.shuffle.service.enabled=false --num-executors 1 --total-executor-cores 1 --executor-cores 1 --executor-memory 2g --driver-memory 2g --name airflow-spark1 /opt/airflow/dags/test.py. Error code is: 1.
[2021-06-06 17:05:49,131] {taskinstance.py:1503} INFO - Marking task as UP_FOR_RETRY. dag_id=forex_data_pipeline, task_id=spark_submit_job, execution_date=20210605T000000, start_date=20210606T165806, end_date=20210606T170549
[2021-06-06 17:05:51,794] {local_task_job.py:146} INFO - Task exited with return code 1
