[2021-06-06 14:20:43,673] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: forex_data_pipeline.spark1 2021-06-05T00:00:00+00:00 [queued]>
[2021-06-06 14:20:43,720] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: forex_data_pipeline.spark1 2021-06-05T00:00:00+00:00 [queued]>
[2021-06-06 14:20:43,721] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2021-06-06 14:20:43,723] {taskinstance.py:1043} INFO - Starting attempt 9 of 10
[2021-06-06 14:20:43,725] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2021-06-06 14:20:43,742] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark1> on 2021-06-05T00:00:00+00:00
[2021-06-06 14:20:43,755] {standard_task_runner.py:52} INFO - Started process 1601 to run task
[2021-06-06 14:20:43,761] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'forex_data_pipeline', 'spark1', '2021-06-05T00:00:00+00:00', '--job-id', '22', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag_spark.py', '--cfg-path', '/tmp/tmpje8pgfqn', '--error-file', '/tmp/tmpy2etp5ky']
[2021-06-06 14:20:43,763] {standard_task_runner.py:77} INFO - Job 22: Subtask spark1
[2021-06-06 14:20:43,852] {logging_mixin.py:104} INFO - Running <TaskInstance: forex_data_pipeline.spark1 2021-06-05T00:00:00+00:00 [running]> on host 5b1a5e9e3fee
[2021-06-06 14:20:43,963] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=admin@localhost.com
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=forex_data_pipeline
AIRFLOW_CTX_TASK_ID=spark1
AIRFLOW_CTX_EXECUTION_DATE=2021-06-05T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-06-05T00:00:00+00:00
[2021-06-06 14:20:43,995] {base.py:74} INFO - Using connection to: id: spark_conn. Host: spark://spark-master, Port: 7077, Schema: , Login: marcelo, Password: XXXXXXXX, extra: None
[2021-06-06 14:20:44,002] {spark_submit.py:364} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --name arrow-spark /opt/airflow/dags/test.py
[2021-06-06 14:20:48,183] {spark_submit.py:526} INFO - WARNING: An illegal reflective access operation has occurred
[2021-06-06 14:20:48,186] {spark_submit.py:526} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/airflow/.local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2021-06-06 14:20:48,188] {spark_submit.py:526} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2021-06-06 14:20:48,189] {spark_submit.py:526} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2021-06-06 14:20:48,192] {spark_submit.py:526} INFO - WARNING: All illegal access operations will be denied in a future release
[2021-06-06 14:20:49,640] {spark_submit.py:526} INFO - 21/06/06 14:20:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-06-06 14:20:51,806] {spark_submit.py:526} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2021-06-06 14:20:51,851] {spark_submit.py:526} INFO - 21/06/06 14:20:51 INFO SparkContext: Running Spark version 3.1.2
[2021-06-06 14:20:52,031] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO ResourceUtils: ==============================================================
[2021-06-06 14:20:52,033] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2021-06-06 14:20:52,035] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO ResourceUtils: ==============================================================
[2021-06-06 14:20:52,037] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO SparkContext: Submitted application: pyspark-run1
[2021-06-06 14:20:52,126] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2021-06-06 14:20:52,173] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO ResourceProfile: Limiting resource is cpu
[2021-06-06 14:20:52,176] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2021-06-06 14:20:52,411] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO SecurityManager: Changing view acls to: airflow
[2021-06-06 14:20:52,414] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO SecurityManager: Changing modify acls to: airflow
[2021-06-06 14:20:52,422] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO SecurityManager: Changing view acls groups to:
[2021-06-06 14:20:52,427] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO SecurityManager: Changing modify acls groups to:
[2021-06-06 14:20:52,429] {spark_submit.py:526} INFO - 21/06/06 14:20:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2021-06-06 14:20:53,090] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO Utils: Successfully started service 'sparkDriver' on port 46695.
[2021-06-06 14:20:53,162] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO SparkEnv: Registering MapOutputTracker
[2021-06-06 14:20:53,242] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO SparkEnv: Registering BlockManagerMaster
[2021-06-06 14:20:53,306] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2021-06-06 14:20:53,308] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2021-06-06 14:20:53,324] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2021-06-06 14:20:53,358] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bec457fc-db8a-4024-b1d8-59d99c4f374f
[2021-06-06 14:20:53,423] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2021-06-06 14:20:53,494] {spark_submit.py:526} INFO - 21/06/06 14:20:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2021-06-06 14:20:54,078] {spark_submit.py:526} INFO - 21/06/06 14:20:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2021-06-06 14:20:54,268] {spark_submit.py:526} INFO - 21/06/06 14:20:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://5b1a5e9e3fee:4040
[2021-06-06 14:20:54,923] {spark_submit.py:526} INFO - 21/06/06 14:20:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2021-06-06 14:20:55,442] {spark_submit.py:526} INFO - 21/06/06 14:20:55 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.4:7077 after 228 ms (0 ms spent in bootstraps)
[2021-06-06 14:20:55,943] {spark_submit.py:526} INFO - 21/06/06 14:20:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210606142055-0003
[2021-06-06 14:20:55,962] {spark_submit.py:526} INFO - 21/06/06 14:20:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210606142055-0003/0 on worker-20210606134554-172.21.0.7-36731 (172.21.0.7:36731) with 1 core(s)
[2021-06-06 14:20:56,007] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20210606142055-0003/0 on hostPort 172.21.0.7:36731 with 1 core(s), 512.0 MiB RAM
[2021-06-06 14:20:56,011] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210606142055-0003/1 on worker-20210606134553-172.21.0.6-46863 (172.21.0.6:46863) with 1 core(s)
[2021-06-06 14:20:56,014] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20210606142055-0003/1 on hostPort 172.21.0.6:46863 with 1 core(s), 512.0 MiB RAM
[2021-06-06 14:20:56,025] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34321.
[2021-06-06 14:20:56,026] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO NettyBlockTransferService: Server created on 5b1a5e9e3fee:34321
[2021-06-06 14:20:56,031] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2021-06-06 14:20:56,067] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5b1a5e9e3fee, 34321, None)
[2021-06-06 14:20:56,093] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO BlockManagerMasterEndpoint: Registering block manager 5b1a5e9e3fee:34321 with 434.4 MiB RAM, BlockManagerId(driver, 5b1a5e9e3fee, 34321, None)
[2021-06-06 14:20:56,097] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5b1a5e9e3fee, 34321, None)
[2021-06-06 14:20:56,104] {spark_submit.py:526} INFO - 21/06/06 14:20:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5b1a5e9e3fee, 34321, None)
[2021-06-06 14:20:56,265] {spark_submit.py:526} INFO - 21/06/06 14:20:56 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2021-06-06 14:20:56,267] {spark_submit.py:526} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2021-06-06 14:20:56,270] {spark_submit.py:526} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2021-06-06 14:20:56,274] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2012)
[2021-06-06 14:20:56,290] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)
[2021-06-06 14:20:56,301] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)
[2021-06-06 14:20:56,305] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)
[2021-06-06 14:20:56,309] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2021-06-06 14:20:56,312] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2021-06-06 14:20:56,316] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2021-06-06 14:20:56,319] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2021-06-06 14:20:56,329] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2021-06-06 14:20:56,334] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:20:56,349] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2021-06-06 14:20:56,357] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2021-06-06 14:20:56,360] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:20:56,363] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2021-06-06 14:20:56,365] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2021-06-06 14:20:56,367] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2021-06-06 14:20:56,370] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2021-06-06 14:20:56,373] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:255)
[2021-06-06 14:20:56,376] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2021-06-06 14:20:56,378] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2021-06-06 14:20:56,383] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2021-06-06 14:20:56,385] {spark_submit.py:526} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2021-06-06 14:20:56,396] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,399] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,406] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,408] {spark_submit.py:526} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2021-06-06 14:20:56,416] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,417] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,419] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,420] {spark_submit.py:526} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2021-06-06 14:20:56,422] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,423] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,426] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,428] {spark_submit.py:526} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2021-06-06 14:20:56,429] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,430] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,431] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,433] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2021-06-06 14:20:56,434] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,436] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,437] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2021-06-06 14:20:56,438] {spark_submit.py:526} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2021-06-06 14:20:56,440] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2021-06-06 14:20:56,441] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2021-06-06 14:20:56,442] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2021-06-06 14:20:56,448] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2021-06-06 14:20:56,453] {spark_submit.py:526} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2021-06-06 14:20:56,456] {spark_submit.py:526} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2021-06-06 14:20:56,457] {spark_submit.py:526} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2021-06-06 14:20:56,459] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 14:20:56,465] {spark_submit.py:526} INFO - 21/06/06 14:20:56 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2021-06-06 14:20:56,469] {spark_submit.py:526} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2021-06-06 14:20:56,475] {spark_submit.py:526} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2021-06-06 14:20:56,477] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2012)
[2021-06-06 14:20:56,479] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)
[2021-06-06 14:20:56,484] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)
[2021-06-06 14:20:56,488] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)
[2021-06-06 14:20:56,496] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2021-06-06 14:20:56,497] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2021-06-06 14:20:56,500] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2021-06-06 14:20:56,502] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2021-06-06 14:20:56,506] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2021-06-06 14:20:56,511] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:20:56,513] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2021-06-06 14:20:56,515] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2021-06-06 14:20:56,517] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:20:56,518] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2021-06-06 14:20:56,520] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2021-06-06 14:20:56,522] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2021-06-06 14:20:56,524] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2021-06-06 14:20:56,525] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:255)
[2021-06-06 14:20:56,526] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2021-06-06 14:20:56,528] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2021-06-06 14:20:56,532] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2021-06-06 14:20:56,533] {spark_submit.py:526} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2021-06-06 14:20:56,535] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,541] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,547] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,551] {spark_submit.py:526} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2021-06-06 14:20:56,552] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,557] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,563] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,569] {spark_submit.py:526} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2021-06-06 14:20:56,575] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,601] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,607] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,609] {spark_submit.py:526} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2021-06-06 14:20:56,616] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,620] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,627] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:20:56,634] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2021-06-06 14:20:56,642] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:20:56,649] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:20:56,655] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2021-06-06 14:20:56,657] {spark_submit.py:526} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2021-06-06 14:20:56,665] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2021-06-06 14:20:56,670] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2021-06-06 14:20:56,671] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2021-06-06 14:20:56,676] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2021-06-06 14:20:56,678] {spark_submit.py:526} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2021-06-06 14:20:56,683] {spark_submit.py:526} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2021-06-06 14:20:56,686] {spark_submit.py:526} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2021-06-06 14:20:56,689] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 14:20:57,236] {spark_submit.py:526} INFO - 21/06/06 14:20:57 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2021-06-06 14:20:59,337] {spark_submit.py:526} INFO - 21/06/06 14:20:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/airflow/spark-warehouse').
[2021-06-06 14:20:59,339] {spark_submit.py:526} INFO - 21/06/06 14:20:59 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2021-06-06 14:21:07,121] {spark_submit.py:526} INFO - 21/06/06 14:21:07 INFO InMemoryFileIndex: It took 792 ms to list leaf files for 1 paths.
[2021-06-06 14:21:07,860] {spark_submit.py:526} INFO - 21/06/06 14:21:07 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.
[2021-06-06 14:21:15,702] {spark_submit.py:526} INFO - 21/06/06 14:21:15 INFO FileSourceStrategy: Pushed Filters:
[2021-06-06 14:21:15,719] {spark_submit.py:526} INFO - 21/06/06 14:21:15 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2021-06-06 14:21:15,732] {spark_submit.py:526} INFO - 21/06/06 14:21:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2021-06-06 14:21:18,934] {spark_submit.py:526} INFO - 21/06/06 14:21:18 INFO CodeGenerator: Code generated in 823.3859 ms
[2021-06-06 14:21:19,343] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 175.1 KiB, free 434.2 MiB)
[2021-06-06 14:21:19,531] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.2 MiB)
[2021-06-06 14:21:19,549] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5b1a5e9e3fee:34321 (size: 27.7 KiB, free: 434.4 MiB)
[2021-06-06 14:21:19,575] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2021-06-06 14:21:19,617] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2021-06-06 14:21:19,932] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2021-06-06 14:21:19,983] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2021-06-06 14:21:19,985] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2021-06-06 14:21:19,989] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO DAGScheduler: Parents of final stage: List()
[2021-06-06 14:21:20,000] {spark_submit.py:526} INFO - 21/06/06 14:21:19 INFO DAGScheduler: Missing parents: List()
[2021-06-06 14:21:20,028] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2021-06-06 14:21:20,231] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.8 KiB, free 434.2 MiB)
[2021-06-06 14:21:20,239] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 434.2 MiB)
[2021-06-06 14:21:20,245] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5b1a5e9e3fee:34321 (size: 5.4 KiB, free: 434.4 MiB)
[2021-06-06 14:21:20,257] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388
[2021-06-06 14:21:20,294] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2021-06-06 14:21:20,298] {spark_submit.py:526} INFO - 21/06/06 14:21:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2021-06-06 14:21:35,383] {spark_submit.py:526} INFO - 21/06/06 14:21:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:21:50,356] {spark_submit.py:526} INFO - 21/06/06 14:21:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:22:05,351] {spark_submit.py:526} INFO - 21/06/06 14:22:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:22:20,351] {spark_submit.py:526} INFO - 21/06/06 14:22:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:22:35,350] {spark_submit.py:526} INFO - 21/06/06 14:22:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:22:50,363] {spark_submit.py:526} INFO - 21/06/06 14:22:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:23:05,353] {spark_submit.py:526} INFO - 21/06/06 14:23:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:23:08,720] {spark_submit.py:526} INFO - 21/06/06 14:23:08 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2021-06-06 14:23:08,723] {spark_submit.py:526} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2021-06-06 14:23:08,724] {spark_submit.py:526} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2021-06-06 14:23:08,728] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2012)
[2021-06-06 14:23:08,730] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)
[2021-06-06 14:23:08,733] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)
[2021-06-06 14:23:08,737] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)
[2021-06-06 14:23:08,739] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2021-06-06 14:23:08,742] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2021-06-06 14:23:08,745] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2021-06-06 14:23:08,748] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2021-06-06 14:23:08,749] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2021-06-06 14:23:08,751] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:08,754] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2021-06-06 14:23:08,760] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2021-06-06 14:23:08,762] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:08,764] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2021-06-06 14:23:08,765] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2021-06-06 14:23:08,771] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2021-06-06 14:23:08,774] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2021-06-06 14:23:08,777] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:255)
[2021-06-06 14:23:08,778] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2021-06-06 14:23:08,791] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2021-06-06 14:23:08,793] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2021-06-06 14:23:08,795] {spark_submit.py:526} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2021-06-06 14:23:08,797] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,799] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,803] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,805] {spark_submit.py:526} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2021-06-06 14:23:08,806] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,809] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,810] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,812] {spark_submit.py:526} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2021-06-06 14:23:08,815] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,819] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,820] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,822] {spark_submit.py:526} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2021-06-06 14:23:08,828] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,831] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,839] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,845] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2021-06-06 14:23:08,848] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,850] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,851] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2021-06-06 14:23:08,853] {spark_submit.py:526} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2021-06-06 14:23:08,857] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2021-06-06 14:23:08,858] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2021-06-06 14:23:08,860] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2021-06-06 14:23:08,862] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2021-06-06 14:23:08,864] {spark_submit.py:526} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2021-06-06 14:23:08,866] {spark_submit.py:526} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2021-06-06 14:23:08,868] {spark_submit.py:526} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2021-06-06 14:23:08,869] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 14:23:08,871] {spark_submit.py:526} INFO - 21/06/06 14:23:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210606142055-0003/2 on worker-20210606134554-172.21.0.7-36731 (172.21.0.7:36731) with 1 core(s)
[2021-06-06 14:23:08,872] {spark_submit.py:526} INFO - 21/06/06 14:23:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20210606142055-0003/2 on hostPort 172.21.0.7:36731 with 1 core(s), 512.0 MiB RAM
[2021-06-06 14:23:08,874] {spark_submit.py:526} INFO - 21/06/06 14:23:08 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2021-06-06 14:23:08,876] {spark_submit.py:526} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2021-06-06 14:23:08,880] {spark_submit.py:526} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2021-06-06 14:23:08,884] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2012)
[2021-06-06 14:23:08,885] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)
[2021-06-06 14:23:08,887] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)
[2021-06-06 14:23:08,889] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)
[2021-06-06 14:23:08,894] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2021-06-06 14:23:08,897] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2021-06-06 14:23:08,899] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2021-06-06 14:23:08,901] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2021-06-06 14:23:08,903] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2021-06-06 14:23:08,905] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:08,909] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2021-06-06 14:23:08,910] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2021-06-06 14:23:08,912] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:08,913] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2021-06-06 14:23:08,916] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2021-06-06 14:23:08,918] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2021-06-06 14:23:08,920] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2021-06-06 14:23:08,922] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:255)
[2021-06-06 14:23:08,924] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2021-06-06 14:23:08,925] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2021-06-06 14:23:08,928] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2021-06-06 14:23:08,929] {spark_submit.py:526} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2021-06-06 14:23:08,931] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,933] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,934] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,936] {spark_submit.py:526} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2021-06-06 14:23:08,937] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,938] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,940] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,942] {spark_submit.py:526} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2021-06-06 14:23:08,943] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,945] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,946] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,948] {spark_submit.py:526} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2021-06-06 14:23:08,950] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,951] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,954] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:08,959] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2021-06-06 14:23:08,962] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:08,964] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:08,966] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2021-06-06 14:23:08,969] {spark_submit.py:526} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2021-06-06 14:23:08,970] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2021-06-06 14:23:08,971] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2021-06-06 14:23:08,973] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2021-06-06 14:23:08,975] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2021-06-06 14:23:08,977] {spark_submit.py:526} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2021-06-06 14:23:08,980] {spark_submit.py:526} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2021-06-06 14:23:08,981] {spark_submit.py:526} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2021-06-06 14:23:08,982] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 14:23:08,988] {spark_submit.py:526} INFO - 21/06/06 14:23:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210606142055-0003/3 on worker-20210606134553-172.21.0.6-46863 (172.21.0.6:46863) with 1 core(s)
[2021-06-06 14:23:08,990] {spark_submit.py:526} INFO - 21/06/06 14:23:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20210606142055-0003/3 on hostPort 172.21.0.6:46863 with 1 core(s), 512.0 MiB RAM
[2021-06-06 14:23:09,167] {spark_submit.py:526} INFO - 21/06/06 14:23:09 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2021-06-06 14:23:09,169] {spark_submit.py:526} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2021-06-06 14:23:09,170] {spark_submit.py:526} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2021-06-06 14:23:09,172] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2012)
[2021-06-06 14:23:09,174] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)
[2021-06-06 14:23:09,175] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)
[2021-06-06 14:23:09,177] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)
[2021-06-06 14:23:09,179] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2021-06-06 14:23:09,181] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2021-06-06 14:23:09,182] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2021-06-06 14:23:09,184] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2021-06-06 14:23:09,186] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2021-06-06 14:23:09,188] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:09,190] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2021-06-06 14:23:09,192] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2021-06-06 14:23:09,193] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:09,196] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2021-06-06 14:23:09,200] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2021-06-06 14:23:09,203] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2021-06-06 14:23:09,204] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2021-06-06 14:23:09,207] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:255)
[2021-06-06 14:23:09,209] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2021-06-06 14:23:09,211] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2021-06-06 14:23:09,213] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2021-06-06 14:23:09,216] {spark_submit.py:526} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2021-06-06 14:23:09,217] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,220] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,222] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,225] {spark_submit.py:526} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2021-06-06 14:23:09,228] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,230] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,233] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,235] {spark_submit.py:526} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2021-06-06 14:23:09,238] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,240] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,242] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,246] {spark_submit.py:526} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2021-06-06 14:23:09,249] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,254] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,257] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,263] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2021-06-06 14:23:09,264] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,267] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,270] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2021-06-06 14:23:09,305] {spark_submit.py:526} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2021-06-06 14:23:09,319] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2021-06-06 14:23:09,323] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2021-06-06 14:23:09,325] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2021-06-06 14:23:09,327] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2021-06-06 14:23:09,333] {spark_submit.py:526} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2021-06-06 14:23:09,337] {spark_submit.py:526} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2021-06-06 14:23:09,340] {spark_submit.py:526} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2021-06-06 14:23:09,343] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 14:23:09,478] {spark_submit.py:526} INFO - 21/06/06 14:23:09 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2021-06-06 14:23:09,479] {spark_submit.py:526} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2021-06-06 14:23:09,481] {spark_submit.py:526} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2021-06-06 14:23:09,482] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2012)
[2021-06-06 14:23:09,484] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)
[2021-06-06 14:23:09,485] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)
[2021-06-06 14:23:09,487] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)
[2021-06-06 14:23:09,489] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2021-06-06 14:23:09,491] {spark_submit.py:526} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2021-06-06 14:23:09,493] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2021-06-06 14:23:09,494] {spark_submit.py:526} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2021-06-06 14:23:09,496] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2021-06-06 14:23:09,498] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:09,500] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2021-06-06 14:23:09,502] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2021-06-06 14:23:09,504] {spark_submit.py:526} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2021-06-06 14:23:09,506] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2021-06-06 14:23:09,507] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2021-06-06 14:23:09,509] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2021-06-06 14:23:09,510] {spark_submit.py:526} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2021-06-06 14:23:09,512] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:255)
[2021-06-06 14:23:09,514] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2021-06-06 14:23:09,515] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2021-06-06 14:23:09,517] {spark_submit.py:526} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2021-06-06 14:23:09,518] {spark_submit.py:526} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2021-06-06 14:23:09,520] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,521] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,523] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,524] {spark_submit.py:526} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2021-06-06 14:23:09,526] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,528] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,531] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,532] {spark_submit.py:526} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2021-06-06 14:23:09,534] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,536] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,539] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,542] {spark_submit.py:526} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2021-06-06 14:23:09,544] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,545] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,547] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2021-06-06 14:23:09,548] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2021-06-06 14:23:09,550] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2021-06-06 14:23:09,552] {spark_submit.py:526} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2021-06-06 14:23:09,553] {spark_submit.py:526} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2021-06-06 14:23:09,554] {spark_submit.py:526} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
[2021-06-06 14:23:09,556] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
[2021-06-06 14:23:09,558] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
[2021-06-06 14:23:09,559] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
[2021-06-06 14:23:09,564] {spark_submit.py:526} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2021-06-06 14:23:09,566] {spark_submit.py:526} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2021-06-06 14:23:09,571] {spark_submit.py:526} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2021-06-06 14:23:09,573] {spark_submit.py:526} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2021-06-06 14:23:09,574] {spark_submit.py:526} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2021-06-06 14:23:20,353] {spark_submit.py:526} INFO - 21/06/06 14:23:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:23:35,351] {spark_submit.py:526} INFO - 21/06/06 14:23:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:23:50,351] {spark_submit.py:526} INFO - 21/06/06 14:23:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:24:05,352] {spark_submit.py:526} INFO - 21/06/06 14:24:05 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:24:20,351] {spark_submit.py:526} INFO - 21/06/06 14:24:20 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:24:35,352] {spark_submit.py:526} INFO - 21/06/06 14:24:35 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:24:50,353] {spark_submit.py:526} INFO - 21/06/06 14:24:50 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2021-06-06 14:24:55,478] {local_task_job.py:188} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2021-06-06 14:24:55,561] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 1601
[2021-06-06 14:24:55,594] {taskinstance.py:1239} ERROR - Received SIGTERM. Terminating subprocesses.
[2021-06-06 14:24:55,670] {spark_submit.py:657} INFO - Sending kill signal to spark-submit
[2021-06-06 14:24:55,839] {process_utils.py:66} INFO - Process psutil.Process(pid=1601, status='terminated', exitcode=0, started='14:20:43') (1601) terminated with exit code 0
[2021-06-06 14:24:56,384] {process_utils.py:66} INFO - Process psutil.Process(pid=1655, status='terminated', started='14:20:50') (1655) terminated with exit code None
[2021-06-06 14:24:57,322] {process_utils.py:66} INFO - Process psutil.Process(pid=1602, status='terminated', started='14:20:44') (1602) terminated with exit code None
[2021-06-06 14:24:57,327] {process_utils.py:66} INFO - Process psutil.Process(pid=1610, status='terminated', started='14:20:44') (1610) terminated with exit code None
